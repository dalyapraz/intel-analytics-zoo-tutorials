{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Tensorflow: Fashion MNIST Dataset\n",
    "\n",
    "Analytics zoo now features the ability to do distributed tensorflow using the Apache Spark framework on Analytics Zoo.  While the BigDL models are powerful in many ways, there is also an advantage to being able to use the better-known Tensorflow engine. \n",
    "\n",
    "Vanilla Tensorflow, however, has some major limitations out of the box.  Parallelization is challenging absent an additional framework that allows us to distribute our workload.  Apache Spark is the perfect such framework -- which is now very mature and able to easily handle heavy workloads on Big Data.\n",
    "\n",
    "This dataset uses the higher-level Keras API to to run the fashion MNIST dataset.\n",
    "\n",
    "The classic MNIST dataset is not a particularly challenging dataset, but its small images size of 28x28 makes it commonly used as a demonstration.  The Fashion MNIST uses the same image size, but instead substitutes items of fashion.  It is a significantly more challenging and interesting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from zoo import init_nncontext\n",
    "sc = init_nncontext()  # create NNContext from Analytics Zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some variables we cn chanage.\n",
    "\n",
    "max_epoch = 5 # How many epochs to run\n",
    "data_num = 6000 # How much data to train on.\n",
    "val_num = 600 # How much data to validate on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "Here we are going to load the data, and convert to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the fashion-mnist pre-shuffled train data and test data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Convert to RDD\n",
    "x_train_rdd = sc.parallelize(x_train[:data_num])\n",
    "y_train_rdd = sc.parallelize(y_train[:data_num])\n",
    "x_test_rdd = sc.parallelize(x_test[:data_num])\n",
    "y_test_rdd = sc.parallelize(y_test[:data_num])\n",
    "training_rdd = x_train_rdd.zip(y_train_rdd)\n",
    "testing_rdd = x_test_rdd.zip(y_test_rdd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Images\n",
    "\n",
    "Here are some sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "\n",
    "for i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    # Display each image\n",
    "    ax.imshow(np.squeeze(x_test[index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text labels\n",
    "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
    "                        \"Trouser\",      # index 1\n",
    "                        \"Pullover\",     # index 2 \n",
    "                        \"Dress\",        # index 3 \n",
    "                        \"Coat\",         # index 4\n",
    "                        \"Sandal\",       # index 5\n",
    "                        \"Shirt\",        # index 6 \n",
    "                        \"Sneaker\",      # index 7 \n",
    "                        \"Bag\",          # index 8 \n",
    "                        \"Ankle boot\"]   # index 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Dataset\n",
    "\n",
    "We will now create a TFDataset, which is a tensorflow dataset, loaded into Apache Spark. This will allow us to use Tensorflow trainign and deployment in a distributed fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.pipeline.api.net import TFDataset\n",
    "dataset = TFDataset.from_rdd(training_rdd,  # Training data\n",
    "                             names=[\"features\", \"labels\"],  # names of keys: features and labels\n",
    "                             shapes=[[28, 28, 1], []],  #Input Shape is image, ouput is a scalar (prediction)\n",
    "                             types=[tf.float32, tf.int32],  #input are floats, output is an integer\n",
    "                             batch_size=280, # Batch size to use in training\n",
    "                             val_rdd=testing_rdd  # What to use as a validation RDD\n",
    "                             )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "\n",
    "This model is a simple, feedforward Neural Network with two hidden layers.\n",
    "\n",
    "We are using Keras API in Tensorflow, which is becoming the new high-level API standard in Tensorflow. This is *not* the Zoo Keras API -- this is the Tensorflow Keras API running on top of a Spark RDD running in BigDL.\n",
    "\n",
    "This is exciting because it allows us to use Zoo's superior distribution capabilities on Spark to perform distributed Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "\n",
    "data = Input(shape=[28, 28, 1])\n",
    "\n",
    "x = Flatten()(data)\n",
    "x = Dense(64, activation='relu')(x)     # Hidden Layer: 64 Neurons\n",
    "x = Dense(64, activation='relu')(x)     # Hidden Layer: 64 Neurons\n",
    "predictions = Dense(10, activation='softmax')(x)  # Output is label 0-9\n",
    "\n",
    "model = Model(inputs=data, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We are going to train using Zoo's TFOptimizer.  This will take the data and train.  We are going to be using RMSProp as our optimizer, with accuracy as our validation metci\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.pipeline.api.net import TFOptimizer\n",
    "from bigdl.optim.optimizer import TrainSummary, ValidationSummary, MaxEpoch\n",
    "\n",
    "\n",
    "optimizer = TFOptimizer.from_keras(model, dataset)\n",
    "\n",
    "optimizer.set_train_summary(TrainSummary(\"/tmp/fashion-mnist_log\", \"mnist\"))\n",
    "optimizer.set_val_summary(ValidationSummary(\"/tmp/fashion-mnist_log\", \"mnist\"))\n",
    "# kick off training\n",
    "optimizer.optimize(end_trigger=MaxEpoch(max_epoch))\n",
    "\n",
    "model.save_weights(\"/tmp/fashion-mnist_keras.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We are going to examine some of the results.  Here we show some sample images and their values.  Red values indicate a misprediction, with the correct index printed afterward, while green values indicate a correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_hat = model.predict(x_test)\n",
    "\n",
    "# Plot a random sample of 10 test images, their predicted labels and ground truth\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "for i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    # Display each image\n",
    "    ax.imshow(np.squeeze(x_test[index]))\n",
    "    predict_index = np.argmax(y_hat[index])\n",
    "    true_index = np.argmax(y_test[index])\n",
    "    # Set the title for each image\n",
    "    ax.set_title(\"{} ({})\".format(fashion_mnist_labels[predict_index], \n",
    "                                  fashion_mnist_labels[true_index]),\n",
    "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Improving the Results\n",
    "\n",
    "We only trained with a samll subset of the data to keep the notebook small and lightweight. However, we could and should train with more data.  We can also certainly train for more epochs!\n",
    "\n",
    "Also, we should get and use a Convolutional Neural Network architecture similar to LeNet, rather than the current Dense Layers in a Multilayer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    " * Distributed Tensorflow on Zoo is an effective way of taking our tensorflow models and distributing them on a cluster.\n",
    " * TFDataset corresponds to the `Dataset` object in the `tf.data` namespace, but is distributed across a spark RDD.\n",
    " * TFOptimizer corresponds to the `Optimizer` objects in Tensorflow\n",
    " * Tensorflow's Keras API `tf.keras` is an execellent choice for working with Analytics Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
