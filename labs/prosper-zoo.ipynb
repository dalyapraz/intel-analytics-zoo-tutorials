{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics ZOO Neural Network : Prosper Loan Dataset\n",
    "\n",
    "We are going to look at the prosper loan dataset.  This dataset shows a history of loans made by Prosper.\n",
    "\n",
    "The idea with this dataset is to try to predict whether a loan will be \"good\" or \"bad.\" There are many ways a loan can be bad, from late payments to defaults, but we will turn this into a binary choice between \"good\" loans and \"bad\" loans.\n",
    "\n",
    "We will use a neural network in the Keras-style API in Analytics Zoo to do the prediction of \"good\" loans or \"bad.\n",
    "\n",
    "This uses Analytics Zoo.   If you want to see a \"vanilla\" Tensorflow version of this lab, click [here](./prosper-tf.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from zoo.common.nncontext import *\n",
    "sc = init_nncontext(\"MPG Regression\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%pylab inline\n",
    "import seaborn\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "We are going to load the data of the prosper loan dataset.\n",
    "\n",
    "Notice we are first loading this into a Pandas dataframe. This is fine for a small dataset, but we will need more than this for a large \"at scale\" notebook.\n",
    "\n",
    "Notice the very large numbers of columns. We are going to take only a selection of those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## small file, start with this\n",
    "#datafile = \"https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data-sample.csv\"\n",
    "## this is a large file\n",
    "datafile = \"https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data.csv.gz\"\n",
    "\n",
    "data = pd.read_csv(datafile)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Selection\n",
    "\n",
    "Let's start with a few columns for now; we can add more later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO : select a few columns \n",
    "## start with: 'LoanStatus',  'EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome', 'ListingCategory'\n",
    "#select_columns = ['LoanStatus', 'EmploymentStatus', 'CreditScore', '???', '???']\n",
    "\n",
    "\n",
    "## we can add more later\n",
    "\n",
    "select_columns = ['LoanStatus',  'EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome', 'ListingCategory']\n",
    "\n",
    "## Note : vector columns can only have Numbers, don't include Categorical columns here\n",
    "## And definitely not 'LoanStatus'  (if you are curious include and see what happens!)\n",
    "vector_columns = [ 'EmpIndex', 'CreditScore', 'StatedMonthlyIncome', 'CategoryIndex']\n",
    "\n",
    "## Feature Columns\n",
    "\n",
    "feature_columns = ['EmploymentStatusFactor', 'CreditScore', 'StatedMonthlyIncome', 'ListingCategoryFactor']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Clean Data\n",
    "\n",
    "We should get rid of the NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO :  Drop any NA, null values.  \n",
    "## Hint : Using `.na.drop()`\n",
    "prosper_clean = data.dropna()\n",
    "\n",
    "print(\"Original record count {:,}, cleaned records count {:,},  dropped {:,}\"\\\n",
    "      .format(len(data), len(prosper_clean), \n",
    "              (len(data) - len(prosper_clean))))\n",
    "prosper_clean.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some summary data\n",
    "\n",
    "Let's look at some summary data here.\n",
    "\n",
    "We'd like to see the cardinality of the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prosper_clean['LoanStatus'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like \"good\" loans outnumber bad about 2-1.  That's a class imbalance but not a dramatic one. We can probably get away with using it as is.\n",
    "\n",
    "Let's now look at the Employment Status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(prosper_clean['EmploymentStatus'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are a variety of different statuses although Full-Time and Employed (which seem to be the same thing), are the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prosper_clean['ListingCategory'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm... the lions share of these appear to be \"debt\" -- not very meaningful.  Many of the more specific categories like \"boat\" and \"RV\" don't account for many loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> What does that say about the cardinality of these categorical columns? ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Converting Categorical columns \n",
    "\n",
    "Convert categorical columns to numeric.   \n",
    "Here let's convert **EmploymentStatus** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use pd.factorize on EmploymentStatus, ListingCategory\n",
    "\n",
    "prosper_clean['EmploymentStatusFactor'] = pd.factorize(prosper_clean['EmploymentStatus'])[0]\n",
    "prosper_clean['ListingCategoryFactor'] = pd.factorize(prosper_clean['ListingCategory'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build feature vectors \n",
    "\n",
    "We're going to use this from the `feature_columns` list.  This will get *only* the feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = prosper_clean[feature_columns]\n",
    "label = prosper_clean['LoanStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split Data into training and test.\n",
    "\n",
    "We will split our the data up into training and test.  (You know the drill by now).\n",
    "\n",
    "**=> TODO: Split dataset into 70% training, 30% validation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO :  Split the data into 70% training and 30% test sets \n",
    "## Hint : 0.7   , 0.3\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, label)\n",
    "print(\"training set = \" , len(train_x))\n",
    "print(\"testing set = \" , len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Neural Network\n",
    "\n",
    "Here we are going to define our network. We will do this with the Analytics Zoo Keras style API.\n",
    "\n",
    "Here's what we have:\n",
    "\n",
    " * Input Layer\n",
    " * Hidden Layer (4 Neurons), with Dropout\n",
    " * Hidden Layer (4 Neurons), with Dropout\n",
    " * Output Layer (1 Neuron)\n",
    " \n",
    " This is *binary* clasification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.pipeline.api.keras.layers import Dense, Dropout\n",
    "from zoo.pipeline.api.keras.models import Sequential\n",
    "\n",
    "def build_model(train_x):\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Dense(4, input_dim=len(train_x.columns)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(4))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(output_dim=1))\n",
    "  \n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "model = build_model(train_x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train. We will do this with the `model.fit()` from analytics zoo. Notice the Keras-like `.fit` semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "print(\"Training begins.\")\n",
    "model.fit(\n",
    "    train_x.values,\n",
    "    train_y.values,\n",
    "    batch_size=20,\n",
    "    nb_epoch=20)\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the model.\n",
    "\n",
    "Let us check to see how the model did, using accuracy as a measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 8: Improve Accuracy\n",
    "\n",
    "### Add more features\n",
    "Look at the schema of the full dataset.  Are there any columns you want to add. Make sure you up the number of neurons in the hidden layer as you add more features."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
