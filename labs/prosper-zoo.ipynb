{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics Zoo Neural Network : Prosper Loan Dataset\n",
    "\n",
    "We are going to look at the prosper loan dataset.  This dataset shows a history of loans made by Prosper.\n",
    "\n",
    "The idea with this dataset is to try to predict whether a loan will be \"good\" or \"bad.\" There are many ways a loan can be bad, from late payments to defaults, but we will turn this into a binary choice between \"good\" loans and \"bad\" loans.\n",
    "\n",
    "We will use a neural network in the Keras-style API in Analytics Zoo to do the prediction of \"good\" loans or \"bad.\n",
    "\n",
    "This uses Analytics Zoo.   If you want to see a \"vanilla\" Tensorflow version of this lab, click [here](./prosper-tf.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from zoo.common.nncontext import init_nncontext\n",
    "sc = init_nncontext(\"Prosper Classification\")  # Initialize the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We are going to load the data of the prosper loan dataset.\n",
    "\n",
    "Notice we are first loading this into a Pandas dataframe. This is fine for a small dataset, but we will need more than this for a large \"at scale\" notebook.\n",
    "\n",
    "Notice the very large numbers of columns. We are going to take only a selection of those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## small file, start with this\n",
    "#datafile = \"https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data-sample.csv\"\n",
    "## this is a large file\n",
    "datafile = \"https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data.csv.gz\"\n",
    "\n",
    "data = pd.read_csv(datafile)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some summary data\n",
    "\n",
    "Let's look at some summary data here.\n",
    "\n",
    "We'd like to see the cardinality of the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['LoanStatus'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like \"good\" loans outnumber bad about 2-1.  That's a class imbalance but not a dramatic one. We can probably get away with using it as is.\n",
    "\n",
    "Let's now look at the Employment Status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data['EmploymentStatus'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are a variety of different statuses although Full-Time and Employed (which seem to be the same thing), are the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Categorical columns \n",
    "\n",
    "Convert categorical columns to numeric.   \n",
    "Here let's convert **EmploymentStatus** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use pd.factorize on EmploymentStatus, ListingCategory\n",
    "\n",
    "data['EmploymentStatusFactor'] = pd.factorize(data['EmploymentStatus'])[0]\n",
    "data['ListingCategoryFactor'] = pd.factorize(data['ListingCategory'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build feature vectors \n",
    "\n",
    "We're going to use this from the `feature_columns` list.  This will get *only* the feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_columns = ['EmploymentStatusFactor', 'CreditScore', 'StatedMonthlyIncome', 'ListingCategoryFactor']\n",
    "features = data[feature_columns]\n",
    "label = data['LoanStatus']\n",
    "features.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into training and test.\n",
    "\n",
    "We will split our the data up into training and test. Here we do 70% training, 30% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Split the data into 70% training and 30% test sets \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, label)\n",
    "print(\"training set = \" , len(train_x))  # Show number of rows intraining set\n",
    "pd.DataFrame(train_x).tail()  # Show last few ros in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test set = \" , len(test_x))  # Show number of rows in test sete\n",
    "pd.DataFrame(test_x).tail()   # Show last few rows of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.keys().tolist())  # Print out the names of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Here we are going to define our network. We will do this with the Analytics Zoo Keras style API.\n",
    "\n",
    "Here's what we have:\n",
    "\n",
    " * Input Layer\n",
    " * Hidden Layer (4 Neurons), with Dropout\n",
    " * Hidden Layer (4 Neurons), with Dropout\n",
    " * Output Layer (1 Neuron)\n",
    " \n",
    "This is *binary* clasification.\n",
    " \n",
    "We will train with Adam optimizer, which is a good all-around optimizer that doesn't need us to play much with hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.pipeline.api.keras.layers import Dense, Dropout\n",
    "from zoo.pipeline.api.keras.models import Sequential\n",
    "\n",
    "def build_model(train_x):\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Dense(4, input_dim=len(train_x.columns), activation='tanh'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(4, activation='tanh'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(output_dim=1, activation='sigmoid'))\n",
    "  \n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "model = build_model(train_x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train. We will do this with the `model.fit()` from analytics zoo. Notice the Keras-like `.fit` semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "print(\"Training begins.\")\n",
    "model.fit(\n",
    "    train_x.values,\n",
    "    train_y.values,\n",
    "    batch_size=500,\n",
    "    nb_epoch=2)\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model.\n",
    "\n",
    "Let us check to see how the model did, using accuracy as a measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "accuracy_score(test_y, np.rint(np.concatenate(predictions.collect())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that we get 67% accuracy, meaning we are right 2/3 of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Improve Accuracy\n",
    "\n",
    "### Add more features\n",
    "Look at the schema of the full dataset.  Are there any columns you want to add? Make sure you up the number of neurons in the hidden layer as you add more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. Models can be used for both Classification and regression\n",
    "2. Classification ususally uses a loss function of cross-entropy rather than MSE.\n",
    "3. A Multilayer Peceptron network can be built with Keras style `Dense` layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
