{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Spark DataFrames and ML Pipeline in Analytics Zoo\n",
    "\n",
    "### Overview\n",
    "\n",
    "Analytics Zoo is well integrated with piplines from Apache Spark Machine Learning (aka MLLib).   The preferred API in Spark is the new (as of 2.x) dataframes-based API, which we will show here.\n",
    "\n",
    "We can use the processing of the \n",
    "\n",
    "\n",
    "### Runtime\n",
    "30 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 -About Data\n",
    "\n",
    "[About data](https://archive.ics.uci.edu/ml/datasets/heart+Disease)\n",
    "\n",
    "This is a classification dataset.  We have a number of patient attributes that wewill use to try to predict the outcome variable, which we are calling `target`.   \n",
    "\n",
    "The input variables are as follows:\n",
    "1. Age (`age`)\n",
    "2. Sex (`sex`)\n",
    "3. CP  (`cp`)\n",
    "4. Resting Blood Pressure (`trestbps`)\n",
    "5. FBS (`fbs`)\n",
    "6. Resting ECG (`restecg`)\n",
    "7. thalach (`thalach`)\n",
    "8. oldpeak (`oldpeak`)\n",
    "9. slope (`slope`)\n",
    "10. ca (`ca`)\n",
    "11. thal (`thal`)\n",
    "\n",
    "\n",
    "Note that the `thal` variable is categorical, having values such as `fixed`, `normal`, or `reversible`.  We are going to need to index this categorical variable.\n",
    "\n",
    "\n",
    "Sample Data:\n",
    "```text\n",
    "age,sex,cp,trestbps,chol,fbs,restecg,thalach,exang,oldpeak,slope,ca,thal,target\n",
    "63,1,1,145,233,1,2,150,0,2.3,3,0,fixed,0\n",
    "67,1,4,160,286,0,2,108,1,1.5,2,3,normal,1\n",
    "67,1,4,120,229,0,2,129,1,2.6,2,2,reversible,0\n",
    "37,1,3,130,250,0,0,187,0,3.5,3,0,normal,0\n",
    "41,0,2,130,204,0,2,172,0,1.4,1,0,normal,0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Init\n",
    "\n",
    "Here we will be initializign Analytics Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zoo\n",
    "from zoo.common.nncontext import init_nncontext\n",
    "\n",
    "sc = init_nncontext(\"Cardiac\")\n",
    "print(\"zoo version : \", zoo.__version__)\n",
    "\n",
    "## Spark UI\n",
    "print('Spark UI running on http://localhost:' + sc.uiWebUrl.split(':')[2])\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Explore Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Load Data\n",
    "\n",
    "We will load the data.  It can be found [here](https://elephantscale-public.s3.amazonaws.com/data/heart/heart.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://elephantscale-public.s3.amazonaws.com/data/heart/heart.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = spark.read.csv(\"heart.csv\", \\\n",
    "                      header=True, inferSchema=True)\n",
    "print(\"record count \", data.count())\n",
    "data = data.na.drop()\n",
    "print (\"clean data count \", data.count())\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Basic Exploration\n",
    "\n",
    "Let's first do a describe funciton and see that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy('target').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Graph\n",
    "\n",
    "Let's look at a breakdown of the frequency of the output.  Based on what we see here, the output is slightly unbalanced, but not dramatically.  We will have to see if our confusion matrix reflects this inbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic frequency graph\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = data.groupBy(\"target\").count().toPandas()\n",
    "print(a)\n",
    "a = a.set_index('target')\n",
    "a.plot(kind='bar', rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Create Feature Vectors\n",
    "\n",
    "Here, we will be "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - No zeroes in Target Label column\n",
    "\n",
    "First, Analytics zoo requires us to have all values in the target column as `1` or hgher.  Since in this case our target column is indexed from zero we will simply add a `1` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyitics-Zoo prefers no 0 (zero) in label column\n",
    "# so we will to add +1 to label\n",
    "data = data.withColumn(\"target2\", data['target']+1)\n",
    "data.groupBy(\"target2\").count().show()\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Columns\n",
    "\n",
    "Here we are going to convert categorical columns using the `StringIndexer` class.  This is not as sophisticated as doing a `OneHotEncoder` or something similar, but it will do for this situation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"thal\", outputCol=\"thalIndex\")\n",
    "data = indexer.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Columns of Interest\n",
    "\n",
    "Now let's select columms of interest:\n",
    "\n",
    "1. Age\n",
    "2. Sex\n",
    "3. trestbps\n",
    "4. chol\n",
    "5. thalach\n",
    "6. oldpeak\n",
    "7. slope\n",
    "8. ca\n",
    "9. thalIndex (the converted thal column from the last step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['age', 'sex', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca', 'thalIndex']\n",
    "target_columns = ['target2']\n",
    "\n",
    "data = data.select(feature_columns + target_columns)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Convert to Double\n",
    "Analytics Zoo likes all numbers in Double. So we will do a simple cast operation to make it that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "# convert everything to double\n",
    "data = data.select([col(c).cast(\"double\") for c in data.columns])\n",
    "data.printSchema()\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Feature Vector\n",
    "\n",
    "Here we will create the feature vector in a new column called `assembled`, using the `VectorAssembler` class and our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "assembler = VectorAssembler (inputCols=feature_columns, outputCol='assembled')\n",
    "fv = assembler.transform(data)\n",
    "fv = fv.withColumn ('label', fv['target2'])\n",
    "fv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Scaling\n",
    "\n",
    "We will use the `StandardScaler` class to standardize all values using Z-Scoring. This will help avoid problems with differing magnitudes of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler (inputCol=\"assembled\", outputCol=\"features\")\n",
    "fv = scaler.fit(fv).transform(fv)\n",
    "fv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fv.printSchema()\n",
    "fv.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Split training / validation\n",
    "\n",
    "We will do a simple train/test split here of 70% training and 30% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, validation) = fv.randomSplit([0.7,0.3])\n",
    "print(\"training set count \", training.count())\n",
    "print(\"validation set count \", validation.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Design Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Designing the network\n",
    "Here's a picture of a simple neural network, like what we have in this example:\n",
    "\n",
    "\n",
    "\n",
    "As you can see, we have a total of 4 layers:\n",
    "\n",
    "1. Input layer (sized as number of features -- in this case 9 : 'a' -- 'h')\n",
    "2. Hidden Layer (size we have to specify as part of the model).\n",
    "3. Hidden Layer (size we will also specify)\n",
    "3. Output Layer (Number of output classes we are trying to classify -- in this case 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step  7 -  Create the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Network HyperParameters\n",
    "\n",
    "Here we are going to set some of of hyperparameters of our network. If we want to tweak things, this is the place to start doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001  \n",
    "training_epochs = 100\n",
    "# batch size should be multiple of number of cores.\n",
    "# So powers of two is a good bet\n",
    "batch_size = 16\n",
    "\n",
    "# Network Parameters\n",
    "n_input = len(feature_columns)\n",
    "n_classes=2\n",
    "n_hidden_1 = 128 # 1st layer number of neurons\n",
    "n_hidden_2 = 128  # 2nd layer number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - setup network\n",
    "\n",
    "Our network here has a total of 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras style APIs to build model\n",
    "from zoo.pipeline.api.keras.models import Sequential\n",
    "from zoo.pipeline.api.keras.layers import Dense\n",
    "\n",
    "\n",
    "## 4 layers = input [9] + first hidden [128]  + second hind layer [128] + output [2]\n",
    "nn = Sequential().add(Dense(n_hidden_1, input_dim=n_input)).\\\n",
    "                  add(Dense(n_hidden_2)).\\\n",
    "                  add(Dense(n_classes, activation=\"log_softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create NNClassifier ML pipleline to train model\n",
    "from bigdl.nn.criterion import ClassNLLCriterion\n",
    "from zoo.pipeline.nnframes import  NNClassifier\n",
    "from zoo.pipeline.api.keras.optimizers import Adam\n",
    "\n",
    "estimator = NNClassifier(nn, ClassNLLCriterion(), [n_input])\n",
    "\n",
    "estimator.setMaxEpoch(training_epochs)\\\n",
    "            .setBatchSize(batch_size)\\\n",
    "            .setLearningRate(learning_rate)\n",
    "\n",
    "estimator.setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "# optimizer method, default is SGD\n",
    "estimator.setOptimMethod(Adam())\n",
    "\n",
    "print (\"nn \\n\", nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Train / Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Train\n",
    "\n",
    "Now let's start training. Notice that our `NNCLassifier` class fron `nnframes` corresponds with the Spark MLLib `estimator.fit` semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## training\n",
    "print (\"starting training...\")\n",
    "model = estimator.fit(training)\n",
    "print(\"initial model training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 -  Prediction\n",
    "\n",
    "Here we are going to call `.transform()` on our model to get our validation data. This will give us a reasonable estimate of how we did.  Naturally, we want to use test data rather than training data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = model.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy(\"prediction\").count().show()\n",
    "predictions.sample(False, 0.1).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Evalauating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 - Basic Eval\n",
    "\n",
    "Let's look at our matching vs missed predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"matching predictions \", predictions.filter(\"prediction == label\").count())\n",
    "print (\"missed predictions \", predictions.filter(\"prediction != label\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 - Accuracy, Precision, AUC\n",
    "\n",
    "Let's get the Accuracy, Precision, and AUC (Area Under the Curve of ROC Curve).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "auPRC = evaluator.evaluate(predictions)\n",
    "print(\"Area under precision-recall curve = \" , auPRC)\n",
    "    \n",
    "recall = MulticlassClassificationEvaluator(metricName=\"weightedRecall\").evaluate(predictions)\n",
    "print(\"recall = \" , recall)\n",
    "\n",
    "precision = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\").evaluate(predictions)\n",
    "print(\"Precision = \", precision)\n",
    "\n",
    "accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\").\\\n",
    "            evaluate(predictions)\n",
    "print(\"accuracy = \",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# we use Spark to calculate confusion matrix as the prediction set can be rather large\n",
    "cm = predictions.groupBy('label').pivot('prediction', [1,2]).count().na.fill(0).orderBy('label')\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "cm_pd = cm.toPandas()\n",
    "# print(cm_pd)\n",
    "cm_pd = cm_pd.set_index('label')  # make 'label' as index\n",
    "# print(cm_pd)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "sn.heatmap(cm_pd, annot=True,fmt='d');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Experiment\n",
    "Try the following :\n",
    "- increase number of hidden layers (3 --> 4 --> 5)\n",
    "- you can also adjust the number of neurons on each \n",
    "\n",
    "See if you can improve the accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
